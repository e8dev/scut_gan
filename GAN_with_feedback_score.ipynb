{"cells":[{"cell_type":"markdown","id":"1bd6775d","metadata":{},"source":["# GAN with feedback score"]},{"cell_type":"code","execution_count":0,"id":"50583bd2","metadata":{},"outputs":[],"source":["### GAN with feedback score\n","### Author: Dmitrii Kuznetsov dk.scut@gmail.com\n","### Original LaTextGAN code forked from https://github.com/GerritBartels/LaTextGAN\n","\n","import sys\n","import ae \n","import ae_training as ae_training\n","import tensorflow as tf\n","import nltk\n","from tqdm import tqdm #show progress bar\n","import json\n","from nltk.tokenize import word_tokenize\n","import re \n","import random\n","import numpy as np\n","import pandas as pd\n","\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","\n","import standard_latextgan as latextgan\n","import standard_latextgan_training as latextgan_training\n","import standard_latextgan_evaluation as eval \n","\n","\n","# DATA\n","print(\"GAN with feedback score\")\n","dataset_prefix = \"aws_music_tools\"\n","save_path = (f\"./_save/{dataset_prefix}\")\n","\n","\n","df_s = pd.read_csv('./data/prepared_sentences.csv')\n","sentences=df_s['reviewText']\n","\n","df_reviews=pd.read_csv('./data/prepared_reviews.csv')\n","review_scores=df_reviews['overall']\n","\n","# Removing/Replacing special characters etc.\n","sentences_clean = [re.sub(r'\\(cont\\)|[\\'’\"]|http\\S+|\\n', '',text.replace(\"\\'\", \"'\").replace(\"&amp\", \"and\")) for text in sentences]\n","sentences_clean = [re.sub(r'\\.\\.+|—+|-+|\\*\\*+', ' ',text) for text in sentences_clean]\n","sentences_clean = [re.sub(r'\\d+\\b', ' <NUM> ', text) for text in sentences_clean]\n","amz_reviews_tokenized = [word_tokenize(text.lower()) for text in sentences_clean]\n","\n","print(f\"Initial after preprocesing: {len(amz_reviews_tokenized)}\")\n","    \n","\n","# REMOVE RARE WORDS\n","\n","# Create a frequency dict of all tokens\n","freqs = {}\n","for text in amz_reviews_tokenized:\n","  for word in text:\n","    freqs[word] = freqs.get(word, 0) + 1 \n","\n","# Removing all words that occurr less than 7 times\n","remove=False\n","cache_text = []\n","cache_score = []\n","for idx, text in enumerate(amz_reviews_tokenized):\n","#for text in amz_reviews_tokenized:\n","  for word in text:\n","    if freqs[word]<7:\n","      remove=True\n","  if remove == False:\n","    cache_text.append(text)\n","    cache_score.append(review_scores[idx])\n","    #print(\"--------\")\n","    #print(text)\n","    #print(review_scores[idx])\n","    #print(\"--------\")\n","  remove=False \n","amz_reviews_tokenized = cache_text\n","amz_reviews_scores = cache_score\n","\n","print()\n","print(f\"Remaining texts after preprocesing: {len(amz_reviews_tokenized)}\")\n","\n","# Add start and end of sequence token to every text\n","# and create the two datasets\n","train_data = []\n","word2vec_data = []\n","\n","for text in amz_reviews_tokenized:\n","    text.insert(len(text), \"<End>\")\n","    text.insert(0, \"<Start>\")\n","    train_data.append((text, text[1:], text[:-1]))\n","    word2vec_data.append(text)\n","\n","max_length = 0\n","idx = 0\n","for text in amz_reviews_tokenized:\n","  if len(text) > max_length:\n","    max_length = len(text)\n","\n","print(f\"Longest text has {max_length} tokens.\")  \n","\n","'''\n","print(amz_reviews_tokenized[0])\n","print(amz_reviews_scores[0])\n","print(\"test\")\n","print(amz_reviews_tokenized[75])\n","print(amz_reviews_scores[75])\n","print(\"test\")\n","'''\n","\n"]},{"cell_type":"markdown","id":"0fed7ec9","metadata":{},"source":["# Word2Vec"]},{"cell_type":"code","execution_count":0,"id":"18714036","metadata":{},"outputs":[],"source":["#Word2Vec\n","word2vec_model = Word2Vec(sentences=word2vec_data, vector_size=200, window=5, min_count=1, workers=4, sg=1, negative=50, epochs = 50)\n","#Save the trained embeddings\n","word2vec_model.save(save_path + \"/skip_gram_embedding_2.model\")\n","\n","\n","# Load previously saved embeddings\n","word2vec_model = Word2Vec.load(save_path + \"/skip_gram_embedding_2.model\")\n","words = list(word2vec_model.wv.index_to_key)\n","#print(words)\n","vocab_size = len(words)\n","print(f\"Vocab size of our word2vec model: {vocab_size}\")"]},{"cell_type":"markdown","id":"1f637e44","metadata":{},"source":["# Preparing Datasets"]},{"cell_type":"code","execution_count":0,"id":"178a0933","metadata":{},"outputs":[],"source":["\n","embedding_matrix = np.zeros((len(words), 200))\n","for i in range(len(words)):\n","    embedding_vector = word2vec_model.wv[word2vec_model.wv.index_to_key[i]]\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Add a single row to shift the matrix to the right (since later we use 0 padding for our batches)\n","embedding_matrix = np.insert(arr=embedding_matrix, obj=0, values=np.zeros(200), axis=0)\n","\n","embedding_matrix.shape\n","\n","word2index_dict = {token: token_index for token_index, token in enumerate(word2vec_model.wv.index_to_key)}\n","\n","text2index_input = []\n","text2index_target = []\n","text2index_teacher_forcing = []\n","\n","\n","# +1 to each index as we use zero paddding and masking (therefore zeros need to be ignored) \n","for input, target, teacher in train_data:\n","  input = [word2index_dict[key]+1 for key in input]\n","  target = [word2index_dict[key]+1 for key in target]\n","  teacher = [word2index_dict[key]+1 for key in teacher]\n","  text2index_input.append(input)\n","  text2index_target.append(target)\n","  text2index_teacher_forcing.append(teacher)\n","\n","\n","\n","# We split the data into train data (90%) and test data (10%)\n","# Ragged Tensors allow us to create tf.Datasets containing different sequence lengths\n","train_ragged_dataset_input = tf.ragged.constant(text2index_input[0:int(len(text2index_input)*0.9)])\n","train_ragged_dataset_target = tf.ragged.constant(text2index_target[0:int(len(text2index_target)*0.9)])\n","train_ragged_dataset_teacher = tf.ragged.constant(text2index_teacher_forcing[0:int(len(text2index_teacher_forcing)*0.9)])\n","#add scores\n","train_ragged_dataset_scores = tf.ragged.constant(amz_reviews_scores[0:int(len(amz_reviews_scores)*0.9)])\n","\n","\n","train_dataset_input = tf.data.Dataset.from_tensor_slices(train_ragged_dataset_input)\n","train_dataset_target = tf.data.Dataset.from_tensor_slices(train_ragged_dataset_target)\n","train_dataset_teacher = tf.data.Dataset.from_tensor_slices(train_ragged_dataset_teacher)\n","#add scores\n","train_dataset_scores = tf.data.Dataset.from_tensor_slices(train_ragged_dataset_scores)\n","\n","\n","# Convert ragged tensors to dense tensor in order to allow us to create padded batches\n","# See: https://github.com/tensorflow/tensorflow/issues/39163\n","train_dataset_input = train_dataset_input.map(lambda x: x)\n","train_dataset_target = train_dataset_target.map(lambda x: x)\n","train_dataset_teacher = train_dataset_teacher.map(lambda x: x)\n","train_dataset_scores = train_dataset_scores.map(lambda x: x)\n","\n","train_dataset = tf.data.Dataset.zip((train_dataset_input, train_dataset_target, train_dataset_teacher, train_dataset_scores)).cache().shuffle(buffer_size=50000, reshuffle_each_iteration=True).padded_batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# Repeat for test data\n","test_ragged_dataset_input = tf.ragged.constant(text2index_input[int(len(text2index_input)*0.9):len(text2index_input)])\n","test_ragged_dataset_target = tf.ragged.constant(text2index_target[int(len(text2index_target)*0.9):len(text2index_target)])\n","test_ragged_dataset_teacher = tf.ragged.constant(text2index_teacher_forcing[int(len(text2index_teacher_forcing)*0.9):len(text2index_teacher_forcing)])\n","#add scores\n","test_ragged_dataset_scores = tf.ragged.constant(amz_reviews_scores[int(len(amz_reviews_scores)*0.9):len(amz_reviews_scores)])\n","\n","\n","test_dataset_input = tf.data.Dataset.from_tensor_slices(test_ragged_dataset_input)\n","test_dataset_target = tf.data.Dataset.from_tensor_slices(test_ragged_dataset_target)\n","test_dataset_teacher = tf.data.Dataset.from_tensor_slices(test_ragged_dataset_teacher)\n","test_dataset_scores = tf.data.Dataset.from_tensor_slices(test_ragged_dataset_scores)\n","\n","test_dataset_input = test_dataset_input.map(lambda x: x)\n","test_dataset_target = test_dataset_target.map(lambda x: x)\n","test_dataset_teacher = test_dataset_teacher.map(lambda x: x)\n","test_dataset_scores = test_dataset_scores.map(lambda x: x)\n","\n","test_dataset = tf.data.Dataset.zip((test_dataset_input, test_dataset_target, test_dataset_teacher, test_dataset_scores)).cache().shuffle(buffer_size=10000, reshuffle_each_iteration=True).padded_batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n"]},{"cell_type":"markdown","id":"f6ff8992","metadata":{},"source":["# Auto encoders training"]},{"cell_type":"code","execution_count":0,"id":"47fd8259","metadata":{},"outputs":[],"source":["amzAE = ae.AutoEncoder(vocab_size=vocab_size, embedding_matrix=embedding_matrix, bidirectional=False)\n","amzAE.compile()\n","\n","ae_training.trainModel(model=amzAE, word2vec_model=word2vec_model, train_dataset=train_dataset, test_dataset=test_dataset, loss_function=tf.keras.losses.SparseCategoricalCrossentropy(), num_epochs=50)\n","amzAE.save_weights(save_path + '/model_weights_ae/ae-epoch-last')\n","\n","#after training we need to load weights to use in GAN\n","#amzAE.load_weights(save_path + '/model_weights_ae/ae-epoch-last')"]},{"cell_type":"markdown","id":"36e63fd8","metadata":{},"source":["# GAN Adversarial training"]},{"cell_type":"code","execution_count":0,"id":"b5a682cb","metadata":{},"outputs":[],"source":["\n","#input is a full text\n","#we send full text to Encoder (part of AE), to get such data to be able to feed Decoder\n","train_dataset_GAN = train_dataset_input\n","train_dataset_GAN = train_dataset_GAN.map(lambda x: tf.squeeze(amzAE.Encoder(tf.expand_dims(x, axis=0))))\n","#train_dataset_GAN = train_dataset_GAN.cache().batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","gan_dataset = tf.data.Dataset.zip((train_dataset_GAN, train_dataset_scores)).cache().batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","#GAN\n","LaTextGAN_Generator = latextgan.Generator()\n","LaTextGAN_Generator.compile()\n","\n","#see text output before training\n","eval.text_generator(generator=LaTextGAN_Generator, autoencoder=amzAE, word2vec_model=word2vec_model, num_texts=20)\n","\n","LaTextGAN_Generator.load_weights(save_path + '/model_weights_latextgan/la-epoch-last')\n","\n","LaTextGAN_Discriminator = latextgan.Discriminator()\n","\n","latextgan_training.train_GAN(\n","  generator=LaTextGAN_Generator, \n","  discriminator=LaTextGAN_Discriminator, \n","  autoencoder=amzAE, \n","  word2vec_model=word2vec_model, \n","  train_dataset_GAN=gan_dataset, \n","  num_epochs=100\n",")\n","\n","LaTextGAN_Generator.save_weights(save_path + \"/model_weights_latextgan/la-epoch-last\")\n"]},{"cell_type":"markdown","id":"5a13fd89","metadata":{},"source":["# Text generation"]},{"cell_type":"code","execution_count":0,"id":"98b3c832","metadata":{},"outputs":[],"source":["eval.text_generator(generator=LaTextGAN_Generator, autoencoder=amzAE, word2vec_model=word2vec_model, num_texts=20)"]}],"metadata":{},"nbformat":4,"nbformat_minor":5}